{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd0e0a83d2837cbcebdc7f204a8b982de19d2adbcdceada914311d93b1426a5ff8f",
   "display_name": "Python 3.8.8 64-bit ('env_conda_1': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "e0a83d2837cbcebdc7f204a8b982de19d2adbcdceada914311d93b1426a5ff8f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timezone, timedelta\n",
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "import re\n",
    "import time as time\n",
    "import os\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(4)\n",
    "tf.random.set_seed(4)"
   ]
  },
  {
   "source": [
    "# Web Scraping\n",
    "We are going to gather our data while putting minimal pressure on the target website's servers. We will use www.rev.com which has many transcripts available. I will mostly focus on the transcripts of Biden and Trump campaign speeches, before the 2020 election"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's give our program some fake credentials to make it appear like a real person is using it\n",
    "headers= {\n",
    "    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "    'accept-encoding': 'gzip, deflate, br',\n",
    "    'accept-language': 'en-US,en;q=0.9,fr;q=0.8,ro;q=0.7,ru;q=0.6,la;q=0.5,pt;q=0.4,de;q=0.3',\n",
    "    'cache-control': 'max-age=0',\n",
    "    'upgrade-insecure-requests': '1',\n",
    "    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36'}\n",
    "\n",
    "# These are the target url's which are mostly from rallys and speeches\n",
    "url_list=[\n",
    "    'https://www.rev.com/blog/transcripts/donald-trump-cpac-2021-speech-transcript',\n",
    "    'https://www.rev.com/blog/transcripts/donald-trump-speech-transcript-departing-white-house-on-inauguration-day',\n",
    "    'https://www.rev.com/blog/transcripts/president-donald-trump-farewell-address-speech-transcript',\n",
    "    'https://www.rev.com/blog/transcripts/donald-trump-second-video-speech-condemning-capitol-violence-transcript',\n",
    "    'https://www.rev.com/blog/transcripts/donald-trumps-first-comments-since-capitol-riots-says-he-wants-no-violence',\n",
    "    'https://www.rev.com/blog/transcripts/donald-trump-concedes-election-condemns-rioters-video-transcript-january-7',\n",
    "    'https://www.rev.com/blog/transcripts/trump-video-telling-protesters-at-capitol-building-to-go-home-transcript',\n",
    "    'https://www.rev.com/blog/transcripts/donald-trump-speech-save-america-rally-transcript-january-6',\n",
    "    'https://www.rev.com/blog/transcripts/donald-trump-rally-speech-transcript-dalton-georgia-senate-runoff-election',\n",
    "    'https://www.rev.com/blog/transcripts/donald-trump-georgia-phone-call-transcript-brad-raffensperger-recording',\n",
    "    'https://www.rev.com/blog/transcripts/donald-trump-hosts-operation-warp-speed-covid-19-vaccine-summit-transcript-december-8',\n",
    "    'https://www.rev.com/blog/transcripts/donald-trump-georgia-rally-transcript-before-senate-runoff-elections-december-5',\n",
    "    'https://www.rev.com/blog/transcripts/donald-trump-speech-on-election-fraud-claims-transcript-december-2',\n",
    "    'https://www.rev.com/blog/transcripts/donald-trump-remarks-transcript-pennsylvania-republican-hearing-on-2020-election',\n",
    "    'https://www.rev.com/blog/transcripts/donald-trump-white-house-press-conference-on-lowering-drug-prices-transcript',\n",
    "    'https://www.rev.com/blog/transcripts/donald-trump-white-house-press-conference-as-election-counts-continue-transcript-november-5',\n",
    "    'https://www.rev.com/blog/transcripts/donald-trump-rally-speech-transcript-kenosha-wi-november-2',\n",
    "    'https://www.rev.com/blog/transcripts/donald-trump-rally-speech-transcript-scranton-pa-november-2',\n",
    "    'https://www.rev.com/blog/transcripts/donald-trump-rally-speech-transcript-fayetteville-nc-november-2',\n",
    "    'https://www.rev.com/blog/transcripts/donald-trump-rally-speech-transcript-rome-georgia-november-1',\n",
    "    'https://www.rev.com/blog/transcripts/donald-trump-rally-speech-transcript-waterford-township-michigan-october-30',\n",
    "    'https://www.rev.com/blog/transcripts/donald-trump-rally-speech-transcript-green-bay-wisconsin-october-30',\n",
    "    'https://www.rev.com/blog/transcripts/donald-trump-rally-speech-transcript-tampa-fl-october-29',\n",
    "    'https://www.rev.com/blog/transcripts/donald-trump-rally-speech-transcript-bullhead-city-az-october-28',\n",
    "    'https://www.rev.com/blog/transcripts/donald-trump-rally-speech-transcript-omaha-ne-october-27',\n",
    "    'https://www.rev.com/blog/transcripts/donald-trump-rally-speech-transcript-west-salem-wisconsin-october-27',\n",
    "    'https://www.rev.com/blog/transcripts/donald-trump-rally-speech-transcript-lansing-michigan-october-27',\n",
    "    'https://www.rev.com/blog/transcripts/donald-trump-circleville-ohio-rally-speech-transcript-october-24'\n",
    "]"
   ]
  },
  {
   "source": [
    "### Our web-scraping function which returns the statements made solely by Donald Trump:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_speeches(url_list):\n",
    "    n=0\n",
    "    global string_all #A global variable containing all of strings, for greater flexibility in the future.\n",
    "    string_all=''\n",
    "    string_Trump=''\n",
    "    for url in url_list:\n",
    "        time.sleep(abs(np.random.normal(28,24))) #I don't want to add extra pressure on their servers. I also don't want anyone else running this code to get their IP address blocked, so I\n",
    "                                                  #added randomness to the timing of our execution via a normal distribution\n",
    "\n",
    "        html=requests.get('{}'.format(url), headers=headers).text #Get the html data\n",
    "        soup=BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        #Let's now get the main blocks of data which are the transcripts:\n",
    "        transcript=soup.find_all('p')\n",
    "\n",
    "        #If the block of text starts with some variation of Donald Trump:, then:\n",
    "        transcript_DT=[statement for statement in transcript if ('Donald Trump:' in statement.text[:30]) | \n",
    "                ('President Trump:' in statement.text[:30]) | ('Donald J. Trump:' in statement.text[:35])] \n",
    "        \n",
    "        for statement in transcript_DT:\n",
    "            string_Trump+='\\n '+re.split('(\\n)', statement.text)[-1] #Use regex to split the sentences on \\n which proceeds Trump:, retrieve Trump's speech. Example given below\n",
    "        for statement in transcript:\n",
    "            string_all+= statement.text\n",
    "        #To output the progress while loading:\n",
    "        n+=1\n",
    "        if n%3==0:\n",
    "            print('{}% completed'.format( round( 100*((url_list.index(url)+1)/len(url_list)), 1) ))\n",
    "\n",
    "    return string_Trump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_Trump=get_speeches(url_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can also load in the text directly, which I saved from earlier\n",
    "def load_Trump_str():\n",
    "    global string_Trump\n",
    "    string_Trump=''\n",
    "    with open('text_Donald.txt','r') as file:\n",
    "        list_Trump=file.readlines()\n",
    "    for line in list_Trump:\n",
    "        string_Trump+=line\n",
    "\n",
    "def write_Trump_str():\n",
    "    global string_Trump\n",
    "    with open('text_Donald.txt', 'w') as file:\n",
    "        file.write(string_Trump)"
   ]
  },
  {
   "source": [
    "## Training our RNN on the data\n",
    "We create a keras Tokenizer which will create a unique number for every word seen in the data. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=False)\n",
    "tokenizer.fit_on_texts([string_Trump])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[104, 1048]]"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences([\"An example\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['an example']"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "tokenizer.sequences_to_texts([[104, 1048]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_id = len(tokenizer.word_index) # number of distinct characters\n",
    "dataset_size=sum(tokenizer.word_counts.values()) #Use for word-level encoding\n",
    "#dataset_size = tokenizer.document_count # total number of characters. Use for char-level"
   ]
  },
  {
   "source": [
    "## Preprocessing Step\n",
    "In order to train our model, we have to create pairs of sets of words. The first set of every pair will be 100 words long (training set), and the second set will contain one word only. Our model will therefore have to determine that, given an input of 100 words (more words for longer term memory), what it expects the output word to be. We will shuffle our sets of data to create an i.i.d dataset. 100 n_steps represents a fairly long memory on which our model will be trained on."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "[encoded] = np.array(tokenizer.texts_to_sequences([string_Trump])) - 1\n",
    "train_size = dataset_size * 100 // 100 #We will not be using a validation set for the time being!\n",
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n",
    "\n",
    "n_steps = 100 #He uses quite lengthy sentences to make his point, so I will set this higher to account for more long term memory\n",
    "window_length = n_steps + 1 # target = input shifted 1 word ahead\n",
    "dataset = dataset.repeat().window(window_length, shift=1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
    "batch_size = 32\n",
    "dataset = dataset.shuffle(7600).batch(batch_size)\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "source": [
    "## Model training\n",
    "We will use a GRU layer instead of LSTM since the GRU is much simpler (and therefore much less memory intensive) and provides very similar precision. Dropout has been shown to improve performance in select cases (especially when using Monte Carlo), and to greatly reduce overfitting. The output will be a softmax function and will have an array of probabilities for each number/word. We will use Adam optimization which uses momentum-like properties to speed up our gradient descent."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_time=datetime.now()\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id],dropout=0.2),\n",
    "    keras.layers.GRU(128, return_sequences=True,dropout=0.2),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation=\"softmax\"))\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
    "history = model.fit(dataset, steps_per_epoch=train_size // batch_size,epochs=10)\n",
    "\n",
    "time_end=datetime.now()-first_time\n",
    "print(time_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save('TrumpBot.h5')"
   ]
  }
 ]
}