{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd0e0a83d2837cbcebdc7f204a8b982de19d2adbcdceada914311d93b1426a5ff8f",
   "display_name": "Python 3.8.8 64-bit ('env_conda_1': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "e0a83d2837cbcebdc7f204a8b982de19d2adbcdceada914311d93b1426a5ff8f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "import tkinter as tk\n",
    "import pickle\n",
    "import re\n",
    "import textwrap\n",
    "from datetime import datetime\n",
    "import numpy as np"
   ]
  },
  {
   "source": [
    "## Loading in our model and tokenizer:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=keras.models.load_model('BidenBot_word_lvl.h5')\n",
    "\n",
    "#Pickling has some security issues so we will retrain our tokenizer on the same dataset:\n",
    "def get_tokenizer():\n",
    "    global tokenizer\n",
    "    global max_id\n",
    "    global dataset_size\n",
    "\n",
    "    string_Biden='' #We will store the Biden dataset here\n",
    "\n",
    "    with open('text_Biden.txt','r') as file:\n",
    "        list_Biden=file.readlines()\n",
    "\n",
    "    for line in list_Biden:\n",
    "        string_Biden+=line\n",
    "\n",
    "    tokenizer = keras.preprocessing.text.Tokenizer(char_level=False)\n",
    "    tokenizer.fit_on_texts([string_Biden])\n",
    "\n",
    "    max_id = len(tokenizer.word_index) # number of distinct characters\n",
    "    dataset_size = tokenizer.document_count # total number of characters\n",
    "\n",
    "get_tokenizer()"
   ]
  },
  {
   "source": [
    "## Data Preprocessing:\n",
    "Similar to what we did for training our RNN model, we will need to use tokenizer to create a list of sequences represented by a sparse one-hot tensor object."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(texts):\n",
    "    X = np.array(tokenizer.texts_to_sequences(texts)) - 1\n",
    "    return tf.one_hot(X, max_id)"
   ]
  },
  {
   "source": [
    "## Creating the output:\n",
    "We will call the complete_text func. with a text variable which acts to initialize our string. \n",
    "For the number of words requested (n_words), we will create said number of characters by predicting the next word, n_words times. To create unique strings, we will use a log function on our predicted probabilities before applying a tf.random.categorical function which uses such values in order to generate a set of values (in this case we only generate one sample). I will show you an example of how temperature affects our output:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "#Generate a random value 40 times\n",
    "temperature=.1\n",
    "probability=.9\n",
    "tf.random.categorical([[np.log(probability)/temperature, np.log(1-probability)/temperature]], num_samples=40).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,\n",
       "        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0]])"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "temperature=5\n",
    "probability=.9\n",
    "tf.random.categorical([[np.log(probability)/temperature, np.log(1-probability)/temperature]], num_samples=40).numpy()"
   ]
  },
  {
   "source": [
    "So as you can see, we are able to directly make our model generate more random output just by increasing the temperature values. Let's apply this below:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_text(text, n_words=100, temperature=1):\n",
    "    for i in range(n_words):\n",
    "        text += next_word(text, temperature)+' '\n",
    "    return text\n",
    "def next_word(text, temperature=1):\n",
    "    X_new = preprocess([text])\n",
    "    y_proba = model(X_new)[0, -1:, :]\n",
    "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
    "    word_id = tf.random.categorical(rescaled_logits, num_samples=1) + 1\n",
    "    return tokenizer.sequences_to_texts(word_id.numpy())[0]"
   ]
  },
  {
   "source": [
    "We will now create functions necessary to run our tkinter code. This will call complete_text(), add new lines with add_n_lines(), so as to not have the output run off the tk window, and capitalize every letter that comes after the end of a sentence. click_generate() will activate when we press the button on our tk window."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def click_generate(entry):\n",
    "    label2['text']=output(entry)\n",
    "\n",
    "def replace_lower_case(speech):\n",
    "    #We will replace all lower case characters after period_space, period_question mark, period_exclamation.\n",
    "    indeces=[i.end() for i in re.finditer('(\\. *)|(\\? *)|(\\! *)', speech)] #Get the indices of the char proceeding a period,...\n",
    "    for i in indeces:\n",
    "        #Separate into three pieces and capitalize the letter. If not a letter then continue,\n",
    "        try:\n",
    "            string_pre=speech[:(i)]\n",
    "            char_Upper=speech[i].capitalize() \n",
    "            string_post=speech[(i+1):]\n",
    "            speech=string_pre+char_Upper+string_post\n",
    "        except Exception as x:\n",
    "            continue\n",
    "    return speech\n",
    "\n",
    "def add_n_lines(speech):\n",
    "    #Add new lines:\n",
    "    string=textwrap.dedent(speech)\n",
    "    speech='\\n'.join(i for line in string.splitlines() for i in textwrap.wrap(line, width=44))\n",
    "    return speech\n",
    "\n",
    "def output(entry):\n",
    "    if len(entry)==0:\n",
    "        return 'Must include text' #If no input given\n",
    "    elif entry[-1]!=' ':\n",
    "        return 'Must end with a space'\n",
    "    if entry.split(' ')[0].lower() not in tokenizer.index_word.values():\n",
    "            return 'Try using a more common word in the start :)'\n",
    "    #You can play around with the temperature yourself. Anything between .01 and 1.0 will give you a relatively good output, with a higher value being more 'random', and with a lower value           allowing the model to more likely choose the word arrangement that it believes to be correct (which often leads to repetition unfortunately)!\n",
    "    speech=complete_text(entry,n_words=84, temperature=0.27)\n",
    "    speech=replace_lower_case(speech)\n",
    "    speech=add_n_lines(str(speech))\n",
    "\n",
    "    speech+= '\\n \\n \\t \\t\\t - BidenBot' #Add a signature\n",
    "    print(speech)\n",
    "    return speech"
   ]
  },
  {
   "source": [
    "Running this may take several minutes, depending on what hardware you are using (especially if you switched to char level encoding)!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Run:\n",
    "Let's now create our window. I have included images for the background, trying to keep it from being political despite the political nature of the training data. Sometimes, if the image doesn't load, you have to close the window and run the code again."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "root=tk.Tk()\n",
    "\n",
    "canvas=tk.Canvas(root, height=750, width=540)\n",
    "canvas.pack()\n",
    "\n",
    "try:\n",
    "    background_image=tk.PhotoImage(file='Biden.png')\n",
    "    background_label=tk.Label(root, image=background_image)\n",
    "    background_label.place(relwidth=1, relheight=1)\n",
    "\n",
    "except Exception as x:\n",
    "    print(x)\n",
    "\n",
    "frame=tk.Frame(root, bg='#1a1aff', bd=4)\n",
    "frame.place(relx=0.5, rely=0.02, relwidth=.9, relheight=0.05, anchor='n')\n",
    "\n",
    "label1=tk.Label(frame, text='Start output with:', bg='#e6ffff', font=30)\n",
    "label1.place(relx=0, rely=0, relwidth=0.3, relheight=1)\n",
    "\n",
    "entry=tk.Entry(frame, bg='#e6ffff', font=50)\n",
    "entry.place(relx=0.3, rely=0, relwidth=.5, relheight=1)\n",
    "\n",
    "button=tk.Button(frame, text='Generate', bg='blue', fg='blue', font=78, command=lambda: click_generate(entry.get()))\n",
    "button.place(relx=.8, rely=0, relwidth=.2, relheight=1)\n",
    "\n",
    "bottom_frame=tk.Frame(root, bg='#1a1aff', bd=4)\n",
    "bottom_frame.place(relx=0.5, rely=.2, relwidth=.8, relheight=.4, anchor='n')\n",
    "\n",
    "label2=tk.Label(bottom_frame, bg='#e6ffff', font=('Microsoft Sans Serif', 20), anchor='nw', justify='left')\n",
    "label2.place(relwidth=1, relheight=1)\n",
    "\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}