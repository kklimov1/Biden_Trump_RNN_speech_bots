{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd0e0a83d2837cbcebdc7f204a8b982de19d2adbcdceada914311d93b1426a5ff8f",
   "display_name": "Python 3.8.8 64-bit ('env_conda_1': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "e0a83d2837cbcebdc7f204a8b982de19d2adbcdceada914311d93b1426a5ff8f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timezone, timedelta\n",
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "import re\n",
    "import time as time\n",
    "import os\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(4)\n",
    "tf.random.set_seed(4)"
   ]
  },
  {
   "source": [
    "# Web Scraping\n",
    "We are going to gather our data while putting minimal pressure on the target website's servers. We will use www.rev.com/ which has many transcripts available. I will mostly focus on the transcripts of Biden and Trump campaign speeches, before the 2020 election"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's give our program some fake credentials to make it appear like a real person is using it\n",
    "headers= {\n",
    "    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "    'accept-encoding': 'gzip, deflate, br',\n",
    "    'accept-language': 'en-US,en;q=0.9,fr;q=0.8,ro;q=0.7,ru;q=0.6,la;q=0.5,pt;q=0.4,de;q=0.3',\n",
    "    'cache-control': 'max-age=0',\n",
    "    'upgrade-insecure-requests': '1',\n",
    "    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36'}\n",
    "\n",
    "# I found that more training data doesn't necessarily mean better quality, so we will try with less (~200,000 character training set) while increasing the number of epochs to train:\n",
    "url_list=[\n",
    "    'https://www.rev.com/blog/transcripts/joe-biden-election-day-remarks-transcript-scranton-pa',\n",
    "    'https://www.rev.com/blog/transcripts/joe-biden-kamala-harris-election-day-eve-rally-speech-transcript-november-2',\n",
    "    'https://www.rev.com/blog/transcripts/joe-biden-campaign-event-speech-transcript-pittsburgh-pa-november-2',\n",
    "    'https://www.rev.com/blog/transcripts/joe-biden-campaign-speech-beaver-county-pennsylvania-november-2',\n",
    "    'https://www.rev.com/blog/transcripts/joe-biden-drive-in-rally-speech-transcript-cleveland-november-2',\n",
    "    'https://www.rev.com/blog/transcripts/joe-biden-campaign-event-speech-transcript-philadelphia-november-1',\n",
    "    'https://www.rev.com/blog/transcripts/joe-biden-barack-obama-campaign-event-speech-transcript-flint-mi-october-31',\n",
    "    'https://www.rev.com/blog/transcripts/joe-biden-campaign-speech-transcript-milwaukee-wisconsin-october-30',\n",
    "    'https://www.rev.com/blog/transcripts/joe-biden-campaign-event-speech-transcript-st-paul-minnesota-october-30',\n",
    "    'https://www.rev.com/blog/transcripts/joe-biden-campaign-event-speech-transcript-des-moines-iowa-october-30',\n",
    "    'https://www.rev.com/blog/transcripts/joe-biden-campaign-rally-speech-transcript-tampa-fl-october-29',\n",
    "    'https://www.rev.com/blog/transcripts/joe-biden-drive-in-rally-speech-transcript-broward-county-fl-october-29',\n",
    "    'https://www.rev.com/blog/transcripts/joe-biden-campaign-speech-transcript-atlanta-georgia-october-27'\n",
    "]"
   ]
  },
  {
   "source": [
    "### Our web-scraping function which returns the statements made solely by Joe Biden:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_speeches(url_list):\n",
    "    n=0\n",
    "    global string_all #A global variable containing all of strings, for greater flexibility in the future.\n",
    "    string_all=''\n",
    "    string_Biden='' #This is where we will store Biden's speeches\n",
    "\n",
    "    for url in url_list:\n",
    "        time.sleep(abs(np.random.normal(40,10))) #I don't want to add extra pressure on their servers. I also don't want anyone else running this code to get their IP address blocked, so I\n",
    "                                                  #added randomness to the timing of our execution via a normal distribution\n",
    "\n",
    "        html=requests.get('{}'.format(url), headers=headers).text #Get the html data\n",
    "        soup=BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        #Let's now get the main blocks of data which are the transcripts:\n",
    "        transcript=soup.find_all('p')\n",
    "\n",
    "        #If the block of text starts with some variation of Joe Biden:, then:\n",
    "        transcript_Biden=[statement for statement in transcript if ('Joe Biden:' in statement.text[:35]) | \n",
    "                ('Joseph R Biden:' in statement.text[:30]) | ('Joseph R. Biden:' in statement.text[:35])] \n",
    "        \n",
    "        #Use regex to split the sentences on \\n which proceeds \"Biden:\", retrieve Biden's speech:\n",
    "        for statement in transcript_Biden:\n",
    "            string_Biden+='\\n '+re.split('(\\n)', statement.text)[-1] \n",
    "        for statement in transcript:\n",
    "            string_all+= statement.text\n",
    "        n+=1\n",
    "        if n%3==0:\n",
    "            print('{}% completed'.format( round( 100*((url_list.index(url)+1)/len(url_list)), 1) ))\n",
    "    print('Done.')\n",
    "    return string_Biden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "23.1% completed\n",
      "46.2% completed\n",
      "69.2% completed\n",
      "92.3% completed\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "string_Biden=get_speeches(url_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can also load in the text directly, which I saved from earlier\n",
    "def load_Biden_str():\n",
    "    global string_Biden\n",
    "    string_Biden=''\n",
    "    with open('text_Biden.txt','r') as file:\n",
    "        list_Biden=file.readlines()\n",
    "    for line in list_Biden:\n",
    "        string_Biden+=line\n",
    "\n",
    "def write_Biden_str():\n",
    "    global string_Biden\n",
    "    with open('text_Biden.txt', 'w') as file:\n",
    "        file.write(string_Biden)"
   ]
  },
  {
   "source": [
    "# Training our RNN on the data\n",
    "First, we train a keras tokenizer on our dataset. This will create an association between every character seen in our data, and a number."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(string_Biden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[2, 30, 5, 15, 19, 11, 2, 1, 6, 7, 19, 13, 3]]"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences([\"Example input\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['e x a m p l e   i n p u t']"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "tokenizer.sequences_to_texts([[2, 30, 5, 15, 19, 11, 2, 1, 6, 7, 19, 13, 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_id = len(tokenizer.word_index) # number of distinct characters\n",
    "dataset_size = tokenizer.document_count # total number of characters"
   ]
  },
  {
   "source": [
    "## Preprocessing Step\n",
    "In order to train our model, we have to create pairs of sets of characters. The first set of every pair will be 100 characters long (training set), and the second set will contain one character only. Our model will therefore have to determine that, given an input of 100 characters (more characters for longer term memory), what it expects the output character to be. We will shuffle our sets of data to create an i.i.d dataset."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "[encoded] = np.array(tokenizer.texts_to_sequences([string_Biden])) - 1\n",
    "train_size = dataset_size * 100 // 100 #We will not be using the validation set for the time being.\n",
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n",
    "\n",
    "n_steps = 100 \n",
    "window_length = n_steps + 1 # target = input shifted 1 character ahead\n",
    "dataset = dataset.repeat().window(window_length, shift=1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
    "batch_size = 32\n",
    "dataset = dataset.shuffle(7600).batch(batch_size)\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.prefetch(1) #Add prefetching"
   ]
  },
  {
   "source": [
    "## Model training\n",
    "We will use a GRU layer instead of LSTM since the GRU is much simpler (and therefore much less memory intensive) and provides very similar precision. Dropout has been shown to improve performance in select cases (especially when using Monte Carlo), and to greatly reduce overfitting. The output will be a softmax function and will have an array of probabilities for each number/char. We will use Adam optimization which uses momentum-like properties to speed up our gradient descent."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/14\n",
      "5821/5821 [==============================] - 1711s 294ms/step - loss: 1.9152\n",
      "Epoch 2/14\n",
      "5821/5821 [==============================] - 1824s 313ms/step - loss: 1.3814\n",
      "Epoch 3/14\n",
      "5821/5821 [==============================] - 1832s 315ms/step - loss: 1.2965\n",
      "Epoch 4/14\n",
      "5821/5821 [==============================] - 1838s 316ms/step - loss: 1.2528\n",
      "Epoch 5/14\n",
      "5821/5821 [==============================] - 1843s 317ms/step - loss: 1.2252\n",
      "Epoch 6/14\n",
      "5821/5821 [==============================] - 1850s 318ms/step - loss: 1.2046\n",
      "Epoch 7/14\n",
      "5821/5821 [==============================] - 1847s 317ms/step - loss: 1.1896\n",
      "Epoch 8/14\n",
      "5821/5821 [==============================] - 1845s 317ms/step - loss: 1.1772\n",
      "Epoch 9/14\n",
      "5821/5821 [==============================] - 1853s 318ms/step - loss: 1.1668\n",
      "Epoch 10/14\n",
      "5821/5821 [==============================] - 1866s 320ms/step - loss: 1.1584\n",
      "Epoch 11/14\n",
      "5821/5821 [==============================] - 1866s 321ms/step - loss: 1.1514\n",
      "Epoch 12/14\n",
      "5821/5821 [==============================] - 1865s 320ms/step - loss: 1.1446\n",
      "Epoch 13/14\n",
      "5821/5821 [==============================] - 1860s 319ms/step - loss: 1.1393\n",
      "Epoch 14/14\n",
      "5821/5821 [==============================] - 1860s 320ms/step - loss: 1.1351\n",
      "7:09:20.601296\n"
     ]
    }
   ],
   "source": [
    "first_time=datetime.now()\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id],dropout=0.2),\n",
    "    keras.layers.GRU(128, return_sequences=True,dropout=0.2),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id,activation=\"softmax\"))\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
    "history = model.fit(dataset, steps_per_epoch=train_size // batch_size,\n",
    "                    epochs=14)\n",
    "\n",
    "time_end=datetime.now()-first_time\n",
    "print(time_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save('BidenBot.h5')"
   ]
  }
 ]
}